{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2f19396-a0f4-4f50-a4cc-62162c42ed72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/Cellar/apache-spark/3.2.1/libexec/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/01 17:37:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/03/01 17:37:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/03/01 17:37:11 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- message: string (nullable = true)\n",
      " |-- results: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/01 17:37:14 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/_p/fncz_94x46n3hpt2_kbtc8540000gn/T/temporary-244d0298-d6b1-42dc-9bcd-159e72d0b648. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/03/01 17:37:14 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "22/03/01 17:37:16 ERROR MicroBatchExecution: Query [id = 07b0a771-c2c0-456d-b10d-67bf05a67470, runId = ed59bfe7-8b2c-46a3-9228-a7cea01cac69] terminated with error\n",
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n",
      "  File \"/Users/tcufer/delo/wordle_pulse/wordle_pulse_venv/lib/python3.8/site-packages/py4j/clientserver.py\", line 581, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "  File \"/Users/tcufer/delo/wordle_pulse/wordle_pulse_venv/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 196, in call\n",
      "    raise e\n",
      "  File \"/Users/tcufer/delo/wordle_pulse/wordle_pulse_venv/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 193, in call\n",
      "    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\n",
      "  File \"/var/folders/_p/fncz_94x46n3hpt2_kbtc8540000gn/T/ipykernel_64516/315727131.py\", line 58, in postgres_sink\n",
      "    with open('secrets.yml', 'r') as file:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'secrets.yml'\n",
      "\n",
      "\tat py4j.Protocol.getReturnValue(Protocol.java:476)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)\n",
      "\tat com.sun.proxy.$Proxy22.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:55)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:35)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:600)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:228)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\n"
     ]
    },
    {
     "ename": "StreamingQueryException",
     "evalue": "An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/Users/tcufer/delo/wordle_pulse/wordle_pulse_venv/lib/python3.8/site-packages/py4j/clientserver.py\", line 581, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File \"/Users/tcufer/delo/wordle_pulse/wordle_pulse_venv/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 196, in call\n    raise e\n  File \"/Users/tcufer/delo/wordle_pulse/wordle_pulse_venv/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 193, in call\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\n  File \"/var/folders/_p/fncz_94x46n3hpt2_kbtc8540000gn/T/ipykernel_64516/315727131.py\", line 58, in postgres_sink\n    with open('secrets.yml', 'r') as file:\nFileNotFoundError: [Errno 2] No such file or directory: 'secrets.yml'\n\n=== Streaming Query ===\nIdentifier: [id = 07b0a771-c2c0-456d-b10d-67bf05a67470, runId = ed59bfe7-8b2c-46a3-9228-a7cea01cac69]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {TextSocketV2[host: localhost, port: 9008]: -1}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nFilter NOT (results#23 = false)\n+- Project [id#8L, created_at#17, user_id#6, message#7, <lambda>(message#7) AS results#23]\n   +- Project [id#8L, to_utc_timestamp(cast(getDate(created_at#9) as timestamp), UTC) AS created_at#17, user_id#6, message#7]\n      +- Project [tweet_data#4.id AS id#8L, tweet_data#4.created_at AS created_at#9, tweet_data#4.user.id_str AS user_id#6, tweet_data#4.text AS message#7]\n         +- Project [from_json(StructField(id,LongType,false), StructField(created_at,StringType,false), StructField(user,StructType(StructField(id_str,StringType,false)),false), StructField(text,StringType,false), value#2, Some(Europe/Ljubljana)) AS tweet_data#4]\n            +- Project [cast(value#0 as string) AS value#2]\n               +- StreamingDataSourceV2Relation [value#0], org.apache.spark.sql.execution.streaming.sources.TextSocketTable$$anon$1@11ba245f, TextSocketV2[host: localhost, port: 9008]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Write to Postgres\u001b[39;00m\n\u001b[1;32m     81\u001b[0m query \u001b[38;5;241m=\u001b[39m filtered_data \\\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;241m.\u001b[39mtrigger(processingTime\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5 seconds\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;241m.\u001b[39mforeachBatch(postgres_sink) \\\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m---> 90\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/delo/wordle_pulse/wordle_pulse_venv/lib/python3.8/site-packages/pyspark/sql/streaming.py:101\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/delo/wordle_pulse/wordle_pulse_venv/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/delo/wordle_pulse/wordle_pulse_venv/lib/python3.8/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/Users/tcufer/delo/wordle_pulse/wordle_pulse_venv/lib/python3.8/site-packages/py4j/clientserver.py\", line 581, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File \"/Users/tcufer/delo/wordle_pulse/wordle_pulse_venv/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 196, in call\n    raise e\n  File \"/Users/tcufer/delo/wordle_pulse/wordle_pulse_venv/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 193, in call\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\n  File \"/var/folders/_p/fncz_94x46n3hpt2_kbtc8540000gn/T/ipykernel_64516/315727131.py\", line 58, in postgres_sink\n    with open('secrets.yml', 'r') as file:\nFileNotFoundError: [Errno 2] No such file or directory: 'secrets.yml'\n\n=== Streaming Query ===\nIdentifier: [id = 07b0a771-c2c0-456d-b10d-67bf05a67470, runId = ed59bfe7-8b2c-46a3-9228-a7cea01cac69]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {TextSocketV2[host: localhost, port: 9008]: -1}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nFilter NOT (results#23 = false)\n+- Project [id#8L, created_at#17, user_id#6, message#7, <lambda>(message#7) AS results#23]\n   +- Project [id#8L, to_utc_timestamp(cast(getDate(created_at#9) as timestamp), UTC) AS created_at#17, user_id#6, message#7]\n      +- Project [tweet_data#4.id AS id#8L, tweet_data#4.created_at AS created_at#9, tweet_data#4.user.id_str AS user_id#6, tweet_data#4.text AS message#7]\n         +- Project [from_json(StructField(id,LongType,false), StructField(created_at,StringType,false), StructField(user,StructType(StructField(id_str,StringType,false)),false), StructField(text,StringType,false), value#2, Some(Europe/Ljubljana)) AS tweet_data#4]\n            +- Project [cast(value#0 as string) AS value#2]\n               +- StreamingDataSourceV2Relation [value#0], org.apache.spark.sql.execution.streaming.sources.TextSocketTable$$anon$1@11ba245f, TextSocketV2[host: localhost, port: 9008]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, from_json, to_date, col, to_utc_timestamp, explode, split\n",
    "from pyspark.sql.types import LongType, StructType, StringType\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pytz\n",
    "import sys\n",
    "import yaml\n",
    "sys.path.append('../')\n",
    "\n",
    "from tweet_parser import TweetParser\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('Wordle score streaming') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9008) \\\n",
    "    .load()\n",
    "\n",
    "## Converting date string format\n",
    "def getDate(x):\n",
    "    if x is not None:\n",
    "        return str(datetime.strptime(x,'%a %b %d %H:%M:%S +0000 %Y').replace(tzinfo=pytz.UTC).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def getResults(text):\n",
    "    return TweetParser(text).wordle_result_exist()\n",
    "\n",
    "## UDF declaration\n",
    "date_fn = udf(getDate, StringType())\n",
    "attempts_fn = udf(lambda  x: getResults(x), StringType())\n",
    "\n",
    "schema = StructType(). \\\n",
    "    add('id', LongType(), False). \\\n",
    "    add('created_at', StringType(), False) .\\\n",
    "    add('user', StructType().add(\"id_str\",StringType(), False), False). \\\n",
    "    add('text', StringType(), False)\n",
    "\n",
    "filtered_data = lines \\\n",
    "    .selectExpr('CAST(value AS STRING)') \\\n",
    "    .select(from_json('value', schema).alias('tweet_data')) \\\n",
    "    .selectExpr('tweet_data.id', 'tweet_data.created_at', 'tweet_data.user.id_str AS user_id', 'tweet_data.text AS message') \\\n",
    "    .withColumn(\"created_at\", to_utc_timestamp(date_fn(\"created_at\"),\"UTC\")) \\\n",
    "    .withColumn('results', attempts_fn(col('message')))\n",
    "\n",
    "filtered_data = filtered_data.filter(col('results') != \"false\")\n",
    "filtered_data.printSchema()\n",
    "\n",
    "    \n",
    "def postgres_sink(df, batch_id):\n",
    "    config = \"\"\n",
    "    with open('secrets.yml', 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "        \n",
    "    dbname = config['dbname']\n",
    "    dbtable = 'tweets'\n",
    "    dbuser = config['dbuser']\n",
    "    dbpass = config['dbpass']\n",
    "    dbhost = config['dbhost']\n",
    "    dbport = config['dbport']\n",
    "\n",
    "    url = \"jdbc:postgresql://\"+dbhost+\":\"+str(dbport)+\"/\"+dbname\n",
    "    properties = {\n",
    "        \"driver\": \"org.postgresql.Driver\",\n",
    "        \"user\": dbuser,\n",
    "        \"password\": dbpass,\n",
    "        \"stringtype\":\"unspecified\"\n",
    "    }\n",
    "    df.write.jdbc(url=url, \n",
    "      table=dbtable, \n",
    "      mode=\"append\",\n",
    "      properties=properties)\n",
    "    \n",
    "# Write to Postgres\n",
    "query = filtered_data \\\n",
    "    .writeStream \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(postgres_sink) \\\n",
    "    .start()\n",
    "\n",
    "\n",
    "    \n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b67fcd-c14c-47df-a7be-f9f394fff5b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordle_pulse_venv",
   "language": "python",
   "name": "wordle_pulse_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
